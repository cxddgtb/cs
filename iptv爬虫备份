import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import re
import os
from datetime import datetime, timedelta, timezone
import time
import opencc
import socket
import json
from concurrent.futures import ThreadPoolExecutor, as_completed

# --- å…¨å±€é…ç½®ä¸åˆå§‹åŒ– ---
timestart = datetime.now()

# --- ä¿®å¤ OpenCC åŠ è½½é—®é¢˜ ---
try:
    # ä½¿ç”¨å†…ç½®é…ç½® 't2s' è€Œä¸æ˜¯æ–‡ä»¶è·¯å¾„
    CC_CONVERTER = opencc.OpenCC('t2s')
    print("âœ… OpenCCè½¬æ¢å™¨å·²æˆåŠŸåŠ è½½å†…ç½®é…ç½® 't2s'")
except Exception as e:
    print(f"âŒ é”™è¯¯: åŠ è½½OpenCCè½¬æ¢å™¨å¤±è´¥ - {e}")
    # å°è¯•ä½¿ç”¨å¤‡é€‰é…ç½®
    try:
        CC_CONVERTER = opencc.OpenCC('s2t.json')
        print("âœ… ä½¿ç”¨å¤‡é€‰é…ç½® 's2t.json' æˆåŠŸ")
    except:
        print("âŒ æ— æ³•åŠ è½½ä»»ä½•OpenCCé…ç½®ï¼Œä½¿ç”¨ç®€ä½“è½¬æ¢åŠŸèƒ½")
        # ç®€æ˜“çš„ç®€ç¹è½¬æ¢ä½œä¸ºå›é€€æ–¹æ¡ˆ
        class FallbackConverter:
            def convert(self, text):
                return text.replace('è‡º', 'å°').replace('ç£', 'æ¹¾')
        CC_CONVERTER = FallbackConverter()

# é…ç½® requests ä¼šè¯
def create_requests_session():
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    })
    retries = Retry(total=3, backoff_factor=0.5, status_forcelist=[500, 502, 503, 504])
    adapter = HTTPAdapter(max_retries=retries)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    return session

HTTP_SESSION = create_requests_session()

# åˆ†ç±»ç³»ç»Ÿé…ç½®
CATEGORY_CONFIG = {
    "main": {
        "ys": {"name": "å¤®è§†é¢‘é“", "keywords": ["cctv", "cntv", "å¤®è§†", "cgtn"], 
               "dictionary": ["CCTV1", "CCTV2", "CCTV3", "CCTV4", "CCTV5", "CCTV5+", "CCTV6", "CCTV7", "CCTV8", "CCTV9", 
                             "CCTV10", "CCTV11", "CCTV12", "CCTV13", "CCTV14", "CCTV15", "CCTV16", "CCTV17", "CCTV4K", "CCTV8K"]},
        "ws": {"name": "å«è§†é¢‘é“", "keywords": ["å«è§†", "weishi", "stv"], 
               "dictionary": ["åŒ—äº¬å«è§†", "æ¹–å—å«è§†", "æµ™æ±Ÿå«è§†", "ä¸œæ–¹å«è§†", "æ±Ÿè‹å«è§†", "å¤©æ´¥å«è§†", "å±±ä¸œå«è§†", "å¹¿ä¸œå«è§†"]},
        "ty": {"name": "ä½“è‚²é¢‘é“", "keywords": ["ä½“è‚²", "sports", "ty", "tiyu"], 
               "dictionary": ["CCTV5", "CCTV5+", "å¹¿ä¸œä½“è‚²", "äº”æ˜Ÿä½“è‚²", "åŒ—äº¬ä½“è‚²"]},
    },
    "international": {
        "jp": {"name": "æ—¥æœ¬é¢‘é“", "keywords": ["æ—¥æœ¬", "japan", "jp", "nhk"], "dictionary": ["NHKç»¼åˆ", "NHKæ•™è‚²", "NHKBS1", "NHKBS4K"]},
        "kr": {"name": "éŸ©å›½é¢‘é“", "keywords": ["éŸ©å›½", "korea", "kr", "kbs"], "dictionary": ["KBS1", "KBS2", "KBSWorld", "MBC", "SBS"]},
    },
    "regional": {
        "sh": {"name": "ä¸Šæµ·é¢‘é“", "keywords": ["ä¸Šæµ·", "shanghai", "sh", "ä¸œæ–¹"], "dictionary": ["ä¸œæ–¹å«è§†", "ä¸Šæµ·æ–°é—»ç»¼åˆ", "ä¸Šæµ·éƒ½å¸‚", "ä¸Šæµ·ä¸œæ–¹å½±è§†"]},
        "bj": {"name": "åŒ—äº¬é¢‘é“", "keywords": ["åŒ—äº¬", "beijing", "bj", "åŒ—äº¬å°"], "dictionary": ["åŒ—äº¬å«è§†", "åŒ—äº¬æ–°é—»", "åŒ—äº¬è´¢ç»", "åŒ—äº¬å½±è§†"]},
    },
    "genre": {
        "yl": {"name": "å¨±ä¹ç»¼åˆ", "keywords": ["å¨±ä¹", "entertain", "yule", "æç¬‘"], "dictionary": ["æ¹–å—å¨±ä¹", "ä¸œæ–¹å¨±ä¹", "æ±Ÿè‹ç»¼è‰º", "æµ™æ±Ÿå¨±ä¹"]},
        "xp": {"name": "å°å“å¤©åœ°", "keywords": ["å°å“", "sketch", "comedy", "æç¬‘"], "dictionary": ["å¤®è§†å°å“", "æ¬¢ä¹å°å“", "å–œå‰§å°å“", "ç»å…¸å°å“"]},
    },
    "specialty": {
        "news": {"name": "æ–°é—»é¢‘é“", "keywords": ["news", "æ–°é—»", "xinwen", "æ—¶æ”¿"], "dictionary": ["æ–°é—»é¢‘é“", "æ—¶æ”¿æ–°é—»", "è´¢ç»æ–°é—»", "å›½é™…æ–°é—»"]},
        "sports": {"name": "ä½“è‚²é¢‘é“", "keywords": ["sports", "ä½“è‚²", "tiyu", "football"], "dictionary": ["ä½“è‚²é¢‘é“", "è¶³çƒ", "ç¯®çƒ", "ç½‘çƒ"]},
    },
    "region": {
        "asia": {"name": "äºšæ´²é¢‘é“", "keywords": ["asia", "äºšæ´²", "yazhou"], "dictionary": ["äºšæ´²å«è§†", "ä¸œäºšé¢‘é“", "ä¸œå—äºšé¢‘é“", "å—äºšé¢‘é“"]},
        "europe": {"name": "æ¬§æ´²é¢‘é“", "keywords": ["europe", "æ¬§æ´²", "ouzhou"], "dictionary": ["æ¬§æ´²å«è§†", "è¥¿æ¬§é¢‘é“", "ä¸œæ¬§é¢‘é“", "åŒ—æ¬§é¢‘é“"]},
    }
}

# é¢‘é“åç§°æ¸…ç†ä¸çº é”™
REMOVAL_LIST = ["ã€ŒIPV4ã€", "ã€ŒIPV6ã€", "[ipv6]", "[ipv4]", "_ç”µä¿¡", "ç”µä¿¡", "ï¼ˆHDï¼‰", "[è¶…æ¸…]", "é«˜æ¸…", "è¶…æ¸…", 
                "-HD", "(HK)", "AKtv", "@", "IPV6", "ğŸï¸", "ğŸ¦", " ", "[BD]", "[VGA]", "[HD]", "[SD]", 
                "(1080p)", "(720p)", "(480p)"]
REPLACEMENTS = {"CCTV-": "CCTV", "CCTV0": "CCTV", "PLUS": "+", "NewTV-": "NewTV", "iHOT-": "iHOT", "NEW": "New", "New_": "New"}

# --- æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---

def read_txt_to_array(file_name):
    try:
        with open(file_name, 'r', encoding='utf-8') as file:
            return [line.strip() for line in file if line.strip()]
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶ '{file_name}' å‡ºé”™: {e}")
        return []

def load_corrections(filename):
    corrections = {}
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip() and not line.startswith('#'):
                    parts = line.strip().split(',')
                    if len(parts) > 1:
                        correct_name = parts[0].strip()
                        for name in parts[1:]:
                            corrections[name.strip()] = correct_name
    except Exception as e:
        print(f"åŠ è½½çº é”™æ–‡ä»¶ '{filename}' å‡ºé”™: {e}")
    return corrections

def fetch_source_content(url):
    try:
        response = HTTP_SESSION.get(url, timeout=15)
        response.raise_for_status()
        
        content = response.content
        text = None
        for encoding in ['utf-8', 'gbk', 'gb2312', 'iso-8859-1', 'big5']:
            try:
                text = content.decode(encoding)
                if text.strip().startswith("#EXTM3U"):
                    text = convert_m3u_to_txt(text)
                return text
            except UnicodeDecodeError:
                continue
        print(f"è­¦å‘Š: æ— æ³•è§£ç æº {url}")
        return None
    except requests.exceptions.RequestException as e:
        print(f"å¤„ç†URLæºå‡ºé”™: {url} ({e})")
        return None

def convert_m3u_to_txt(m3u_content):
    lines = m3u_content.strip().split('\n')
    txt_lines = []
    channel_name = ""
    for line in lines:
        line = line.strip()
        if line.startswith("#EXTM3U"):
            continue
        if line.startswith("#EXTINF"):
            channel_name = line.split(',')[-1].strip()
        elif line.startswith("http") or line.startswith("rtmp"):
            if channel_name:
                txt_lines.append(f"{channel_name},{line}")
                channel_name = ""
        elif "#genre#" not in line and "," in line and re.match(r'^[^,]+,[^\s]+://[^\s]+$', line):
            txt_lines.append(line)
    return '\n'.join(txt_lines)

def parse_and_clean_channels(source_content, corrections_name):
    channels = set()
    if not source_content:
        return channels
    
    lines = source_content.strip().split('\n')
    for line in lines:
        line = line.strip()
        if not line or "#genre#" in line or "#EXTINF:" in line:
            continue
        
        try:
            parts = line.split(',', 1)
            if len(parts) < 2:
                continue
                
            channel_name = parts[0].strip()
            channel_url = parts[1].split('$')[0].strip()

            if not channel_url:
                continue
            
            # ä½¿ç”¨ OpenCC è½¬æ¢
            try:
                simplified_name = CC_CONVERTER.convert(channel_name)
            except Exception as e:
                print(f"OpenCCè½¬æ¢å¤±è´¥: {channel_name} - {e}")
                simplified_name = channel_name
            
            cleaned_name = simplified_name
            for item in REMOVAL_LIST:
                cleaned_name = cleaned_name.replace(item, "")
            for old, new in REPLACEMENTS.items():
                cleaned_name = cleaned_name.replace(old, new)
            
            corrected_name = corrections_name.get(cleaned_name.strip(), cleaned_name.strip())
            
            channels.add((corrected_name, channel_url))
        except Exception as e:
            print(f"è§£æé¢‘é“è¡Œå¤±è´¥: {line} - {e}")
    return channels

def check_channel_availability(channel_info, timeout=2):
    name, url = channel_info
    if "127.0.0.1" in url or "localhost" in url:
        return name, url, 0

    try:
        start_time = time.time()
        response = HTTP_SESSION.head(url, timeout=timeout, allow_redirects=True)
        end_time = time.time()
        latency = int((end_time - start_time) * 1000)
        if response.status_code < 400:
            return name, url, latency
        return name, url, -1
    except Exception:
        return name, url, -1

def classify_channel(channel_name, channel_url):
    channel_name_lower = channel_name.lower()
    
    if "æ˜¥æ™š" in channel_name: return "cw"
    if "ç›´æ’­ä¸­å›½" in channel_name: return "zb"
    if "mtv" in channel_name_lower: return "mtv"
    if "radio" in channel_name_lower or "å¹¿æ’­" in channel_name or "fm" in channel_name_lower: return "radio"
    
    for category_type in CATEGORY_CONFIG.values():
        for category_id, info in category_type.items():
            for keyword in info.get("dictionary", []):
                if keyword in channel_name:
                    return category_id
            for keyword in info.get("keywords", []):
                if keyword in channel_name_lower:
                    return category_id
    return "other"

def sort_data(order, data):
    order_dict = {name: i for i, name in enumerate(order)}
    return sorted(data, key=lambda line: order_dict.get(line.split(',')[0], len(order)))

def save_files(final_lists):
    all_lines = []
    utc_time = datetime.now(timezone.utc)
    beijing_time = utc_time + timedelta(hours=8)
    version = f"{beijing_time.strftime('%Y%m%d %H:%M')},https://gcalic.v.myalicdn.com/gc/wgw05_1/index.m3u8?contentid=2820180516001"
    all_lines.extend([f"æ›´æ–°æ—¶é—´,#genre#", version, ''])

    total_channels = 0

    def add_category(name, lines, dictionary=None):
        nonlocal total_channels
        if lines:
            all_lines.append(f"{name},#genre#")
            sorted_lines = sort_data(dictionary, lines) if dictionary else sorted(lines)
            all_lines.extend(sorted_lines)
            all_lines.append('')
            total_channels += len(lines)

    for category_type in CATEGORY_CONFIG.values():
        for cat_id, info in category_type.items():
            if final_lists[cat_id]:
                add_category(info["name"], final_lists[cat_id], info.get("dictionary"))
    
    add_category("æ˜¥æ™š", final_lists['cw'])
    add_category("ç›´æ’­ä¸­å›½", final_lists['zb'])
    add_category("MTV", final_lists['mtv'])
    add_category("æ”¶éŸ³æœºé¢‘é“", final_lists['radio'])
    add_category("å…¶ä»–é¢‘é“", final_lists['other'])

    final_content = "\n".join(all_lines)

    try:
        with open("live.txt", "w", encoding='utf-8') as f:
            f.write(final_content)
        print("âœ… é¢‘é“æ–‡ä»¶å·²ä¿å­˜: live.txt")
    except Exception as e:
        print(f"âŒ ä¿å­˜ live.txt å‡ºé”™: {e}")

    try:
        m3u_content = '#EXTM3U x-tvg-url="https://epg.112114.xyz/pp.xml.gz"\n'
        group_title = ""
        for line in final_content.strip().split('\n'):
            if not line.strip(): continue
            if "#genre#" in line:
                group_title = line.split(',')[0]
                continue
            if "," in line:
                name, url = line.split(',', 1)
                logo = f"https://epg.112114.xyz/logo/{name}.png"
                m3u_content += f'#EXTINF:-1 tvg-name="{name}" tvg-logo="{logo}" group-title="{group_title}",{name}\n{url}\n'
        with open("live.m3u", "w", encoding='utf-8') as f:
            f.write(m3u_content)
        print("âœ… M3Uæ–‡ä»¶å·²ä¿å­˜: live.m3u")
    except Exception as e:
        print(f"âŒ ç”Ÿæˆ live.m3u å‡ºé”™: {e}")

    return total_channels

# --- ä¸»æ‰§è¡Œæµç¨‹ ---

def main():
    print("â¡ï¸ æ­¥éª¤ 1/5: åŠ è½½æœ¬åœ°èµ„æº (URLs, çº é”™è¯å…¸)...")
    assets_dir = 'assets'
    urls_file = os.path.join(assets_dir, 'urls.txt')
    corrections_file = os.path.join(assets_dir, 'corrections_name.txt')

    urls_to_process = read_txt_to_array(urls_file)
    corrections = load_corrections(corrections_file)

    if not urls_to_process:
        print(f"âŒ é”™è¯¯: '{urls_file}' ä¸ºç©ºæˆ–ä¸å­˜åœ¨ï¼Œç¨‹åºé€€å‡ºã€‚")
        return

    print(f"\nâ¡ï¸ æ­¥éª¤ 2/5: å¹¶å‘è·å– {len(urls_to_process)} ä¸ªåœ¨çº¿æº...")
    all_raw_channels = set()
    with ThreadPoolExecutor(max_workers=20) as executor:
        future_to_url = {executor.submit(fetch_source_content, url): url for url in urls_to_process}
        for future in as_completed(future_to_url):
            url = future_to_url[future]
            try:
                content = future.result()
                if content:
                    parsed_channels = parse_and_clean_channels(content, corrections)
                    all_raw_channels.update(parsed_channels)
                    print(f"âœ“ æˆåŠŸå¤„ç†: {url}")
                else:
                    print(f"âš ï¸ ç©ºå†…å®¹: {url}")
            except Exception as e:
                print(f"âŒ å¤„ç†æºå¤±è´¥: {url} - {e}")
    
    print(f"\nâœ… æˆåŠŸè·å–å¹¶è§£æäº† {len(all_raw_channels)} ä¸ªä¸é‡å¤çš„é¢‘é“ã€‚")

    if not all_raw_channels:
        print("âŒ é”™è¯¯: æ²¡æœ‰è·å–åˆ°ä»»ä½•é¢‘é“ï¼Œç¨‹åºé€€å‡º")
        return

    print(f"\nâ¡ï¸ æ­¥éª¤ 3/5: å¹¶å‘æ£€æµ‹ {len(all_raw_channels)} ä¸ªé¢‘é“çš„æœ‰æ•ˆæ€§...")
    valid_channels = []
    with ThreadPoolExecutor(max_workers=50) as executor:
        future_to_channel = {executor.submit(check_channel_availability, channel): channel for channel in all_raw_channels}
        
        processed_count = 0
        total_count = len(future_to_channel)
        for future in as_completed(future_to_channel):
            processed_count += 1
            try:
                name, url, latency = future.result()
                if latency >= 0:
                    valid_channels.append((name, url))
                    status = "âœ“" if latency >= 0 else "âœ—"
                    print(f"\r  - [{processed_count}/{total_count}] {status} {name[:20]}... å»¶è¿Ÿ: {latency}ms", end="")
            except Exception as e:
                print(f"\nâŒ æ£€æµ‹å¤±è´¥: {e}")
    
    print(f"\n\nâœ… æ£€æµ‹å®Œæˆï¼Œå‘ç° {len(valid_channels)} ä¸ªæœ‰æ•ˆé¢‘é“ã€‚")

    print("\nâ¡ï¸ æ­¥éª¤ 4/5: å¯¹æœ‰æ•ˆé¢‘é“è¿›è¡Œåˆ†ç±»...")
    categorized_lists = {cat_id: [] for cat_type in CATEGORY_CONFIG.values() for cat_id in cat_type}
    categorized_lists.update({'cw': [], 'zb': [], 'mtv': [], 'radio': [], 'other': []})

    for name, url in valid_channels:
        category = classify_channel(name, url)
        categorized_lists[category].append(f"{name},{url}")
    print("âœ… åˆ†ç±»å®Œæˆã€‚")

    print("\nâ¡ï¸ æ­¥éª¤ 5/5: ç”Ÿæˆæœ€ç»ˆæ–‡ä»¶å¹¶ç»Ÿè®¡ç»“æœ...")
    total_saved = save_files(categorized_lists)

    print("\n--- ä»»åŠ¡å®Œæˆ ---")
    timeend = datetime.now()
    elapsed = timeend - timestart
    minutes, seconds = divmod(int(elapsed.total_seconds()), 60)
    print(f"ğŸ“Š æ€»è€—æ—¶: {minutes}åˆ† {seconds}ç§’")
    print(f"ğŸ“Š æ€»è®¡æœ‰æ•ˆé¢‘é“æ•°: {total_saved}")

if __name__ == "__main__":
    main()
